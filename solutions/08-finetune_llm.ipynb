{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08: Finetune LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "import random\n",
    "# import os\n",
    "# CACHE_PATH = '<path_to_cache>'\n",
    "# os.environ[\"TORCH_HOME\"] = CACHE_PATH\n",
    "# os.environ[\"HF_HOME\"] = CACHE_PATH\n",
    "# os.environ[\"HUGGINGFACE_HUB_CACHE\"] = CACHE_PATH\n",
    "# os.environ[\"HUGGINGFACE_ASSETS_CACHE\"] = CACHE_PATH\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_PATH\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "# from huggingface_hub import login; login(token=\"<hf_token>\")\n",
    "\n",
    "import pruna_pro\n",
    "from pruna_pro import SmashConfig\n",
    "from pruna_pro import smash\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics.metric_elapsed_time import ElapsedTimeMetric\n",
    "from pruna.evaluation.metrics.metric_torch import TorchMetricWrapper\n",
    "from pruna.evaluation.metrics.metric_energy import EnergyMetric\n",
    "from pruna.evaluation.metrics.metric_memory import GPUMemoryMetric\n",
    "from pruna.evaluation.metrics.metric_model_architecture import ModelArchitectureMetric\n",
    "from pruna.evaluation.task import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utils\n",
    "\n",
    "The utils functions help for:\n",
    "- Load from a list of (small) models.  eel free to try other models until the GPU memory is not enough!\n",
    "- Make plots.\n",
    "- Iterate over evaluation and model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\", \n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"facebook/opt-2.7b\",\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    # \"microsoft/Phi-4-mini-instruct\",\n",
    "    # \"HuggingFaceTB/SmolLM-135M\",\n",
    "    # \"HuggingFaceTB/SmolLM-135M-instruct\",\n",
    "    # \"HuggingFaceTB/SmolLM-360M\", \n",
    "    # \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "    # \"HuggingFaceTB/SmolLM-1.7B\",\n",
    "    # \"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n",
    "    # \"HuggingFaceTB/SmolLM2-135M\",\n",
    "    # \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    "    # \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    # \"HuggingFaceTB/SmolLM2-360M-Instruct\", \n",
    "    # \"HuggingFaceTB/SmolLM2-1.7B\",\n",
    "    # \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "    # \"PleIAs/Pleias-350m-Preview\",\n",
    "    # \"PleIAs/Pleias-Pico\",\n",
    "    # \"PleIAs/Pleias-1.2b-Preview\",\n",
    "    # \"PleIAs/Pleias-Nano\",\n",
    "    # \"PleIAs/Pleias-3b-Preview\",\n",
    "]\n",
    "\n",
    "# Load model and tokenizer for first model in list\n",
    "model_id = model_ids[1]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune LLM\n",
    "\n",
    "We recommend to checkout the [Pruna documentation](https://pruna.readthedocs.io/en/latest/index.html) for access to AI efficiency functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluate the base model quality\n",
    "\n",
    "**Implementation task:**\n",
    "- Evaluate the base model quality and latency with the perplexity metric on the WikiText dataset.\n",
    "- Repeat the experiment with other LLMs and/or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smash_evaluate_perplexity_time(model, tokenizer, smash_config, dataset=\"WikiText\"):\n",
    "    ### To Complete ###\n",
    "    model_copy = copy.deepcopy(model)\n",
    "\n",
    "    if smash_config:\n",
    "        model_copy = smash(model_copy, smash_config)\n",
    "    metrics = [\n",
    "    ElapsedTimeMetric(n_iterations = 100,\n",
    "                    n_warmup_iterations = 10,\n",
    "                    device = \"cuda\",\n",
    "                    timing_type = \"sync\",),\n",
    "    TorchMetricWrapper(metric_name=\"perplexity\", call_type=\"y_gt\")\n",
    "    ]\n",
    "    task = Task(metrics, datamodule=PrunaDataModule.from_string(dataset, tokenizer=tokenizer))\n",
    "    eval_agent = EvaluationAgent(task)\n",
    "    results = eval_agent.evaluate(model_copy)\n",
    "    \n",
    "    del model_copy\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ### End of To Complete ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f4955a23370>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a base model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity_y_gt': 39.13822937011719, 'inference_elapsed_time_ms_@1': 1270.105492591858, 'inference_latency_ms_@1': 12.701054925918578, 'inference_throughput_batches_per_ms_@1': 0.07873361746978486}\n"
     ]
    }
   ],
   "source": [
    "### To Complete ###\n",
    "results = smash_evaluate_perplexity_time(model, tokenizer, None, dataset=\"WikiText\")\n",
    "print(results)\n",
    "### End of To Complete ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finetune LLM quality with in-distribution data\n",
    "\n",
    "**Implementation task:**\n",
    "- Finetune in-place or by adding paramters a quantized LLM with Quanto and evaluate its quality and latency metric on the WikiText dataset.\n",
    "\n",
    "**Questions:**\n",
    "- Is there a performance improvement after finetuning?\n",
    "- Do you observe a latency difference? How could you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f4955a23370>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "INFO - Verifying Pruna token.\n",
      "INFO - You have used 345 hours this month.\n",
      "INFO - Starting quantizer quanto...\n",
      "INFO - quantizer quanto was applied successfully.\n",
      "INFO - Starting recoverer text_to_text_inplace_perp...\n",
      "INFO - You have used 345 hours this month.\n",
      "WARNING - Skipped 2.57e+07 head parameters that were not trainable due to quantization.\n",
      "INFO - Recovering with PERP: norm + bias, totaling 3.19e+05 parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4f198981134012b7e77d414c0681c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffecda688f794359aab2e11c2fd4f3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82c58bfd2514f3d81d415fe157f2352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dabdc9145b49a4a5b728a82317468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17556' max='17556' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17556/17556 58:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2194</td>\n",
       "      <td>3.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4388</td>\n",
       "      <td>3.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6582</td>\n",
       "      <td>3.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8776</td>\n",
       "      <td>3.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10970</td>\n",
       "      <td>3.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13164</td>\n",
       "      <td>3.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15358</td>\n",
       "      <td>3.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17552</td>\n",
       "      <td>3.691700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - recoverer text_to_text_inplace_perp was applied successfully.\n",
      "INFO - You have used 348 hours this month.\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f4955a23370>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity_y_gt': 29.485769271850586, 'inference_elapsed_time_ms_@1': 5949.783557891846, 'inference_latency_ms_@1': 59.497835578918455, 'inference_throughput_batches_per_ms_@1': 0.01680733408652473}\n"
     ]
    }
   ],
   "source": [
    "### To Complete ###\n",
    "smash_config = SmashConfig()\n",
    "smash_config.add_tokenizer(model_id)\n",
    "smash_config.add_data(\"WikiText\", tokenizer=tokenizer)\n",
    "smash_config['quantizer'] = 'quanto'\n",
    "smash_config['quanto_weight_bits'] = \"qint4\"\n",
    "smash_config['recoverer'] = \"text_to_text_inplace_perp\"\n",
    "# smash_config['recoverer'] = \"text_to_text_perp\"\n",
    "model = model.to('cuda')\n",
    "\n",
    "results = smash_evaluate_perplexity_time(model, tokenizer, smash_config, dataset=\"WikiText\")\n",
    "print(results)\n",
    "### End of To Complete ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Finetune LLM quality with more/less in-distribution data\n",
    "\n",
    "**Implementation task:**\n",
    "- Finetune in-place or by adding paramters a quantized LLM with Quanto and evaluate its quality and latency metric on the WikiText dataset.\n",
    "\n",
    "**Questions:**\n",
    "- Is there a performance improvement after finetuning?\n",
    "- Do you observe a latency difference? How could you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using max_seq_len of tokenizer: None\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f4955a23370>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "INFO - Verifying Pruna token.\n",
      "INFO - You have used 348 hours this month.\n",
      "INFO - Starting quantizer quanto...\n",
      "INFO - quantizer quanto was applied successfully.\n",
      "INFO - Starting recoverer text_to_text_inplace_perp...\n",
      "INFO - You have used 348 hours this month.\n",
      "WARNING - Skipped 2.57e+07 head parameters that were not trainable due to quantization.\n",
      "INFO - Recovering with PERP: norm + bias, totaling 3.19e+05 parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae9fa4349324c68a50aeafd91471bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808f7fe6e0c24c038e5998cab393f48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ea5cfbf7194c6da49a5a3fabca6671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8bffae07814c1f86b6a3f860ac3bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>4.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>4.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>3.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.956700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>3.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.809000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - recoverer text_to_text_inplace_perp was applied successfully.\n",
      "INFO - You have used 348 hours this month.\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f4955a23370>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity_y_gt': 35.11149215698242, 'inference_elapsed_time_ms_@1': 6988.028305053711, 'inference_latency_ms_@1': 69.88028305053712, 'inference_throughput_batches_per_ms_@1': 0.01431018817248929}\n"
     ]
    }
   ],
   "source": [
    "### To Complete ###\n",
    "train_ds, val_ds, test_ds = load_dataset(\"mikasenghaas/wikitext-2\", split=[\"train\", \"validation\", \"test\"])\n",
    "train_ds = train_ds.select(range(1000))\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config.add_tokenizer(model_id)\n",
    "smash_config.add_data(\n",
    "    (train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"text_generation_collate\"\n",
    ")\n",
    "smash_config['quantizer'] = 'quanto'\n",
    "smash_config['quanto_weight_bits'] = \"qint4\"\n",
    "smash_config['recoverer'] = \"text_to_text_inplace_perp\"\n",
    "# smash_config['recoverer'] = \"text_to_text_perp\"\n",
    "\n",
    "results = smash_evaluate_perplexity_time(model, tokenizer, smash_config, dataset=\"WikiText\")\n",
    "print(results)\n",
    "### End of To Complete ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Finetune LLM quality with random data\n",
    "\n",
    "**Implementation task:**\n",
    "- Finetune in-place or by adding paramters a quantized LLM with Quanto and evaluate its quality and latency metric on the WikiText dataset.\n",
    "\n",
    "**Questions:**\n",
    "- Is there a performance improvement after finetuning?\n",
    "- Do you observe a latency difference? How could you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f8538f52dd0>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "INFO - Verifying Pruna token.\n",
      "INFO - You have used 359 hours this month.\n",
      "INFO - Starting quantizer quanto...\n",
      "INFO - quantizer quanto was applied successfully.\n",
      "INFO - Starting recoverer text_to_text_inplace_perp...\n",
      "INFO - You have used 359 hours this month.\n",
      "WARNING - Skipped 2.57e+07 head parameters that were not trainable due to quantization.\n",
      "INFO - Recovering with PERP: norm + bias, totaling 3.19e+05 parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df15c7af60184e3c8ca80d6216feab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc9b9fa19424321b32856395293dc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb16c02b0282404496564e94e454c947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b597c70f3e47dca449e937e84aff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 01:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.867800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - recoverer text_to_text_inplace_perp was applied successfully.\n",
      "INFO - You have used 360 hours this month.\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Using max_seq_len of tokenizer: None\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f8538f52dd0>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity_y_gt': 167.92860412597656, 'inference_elapsed_time_ms_@1': 3896.4342079162598, 'inference_latency_ms_@1': 38.9643420791626, 'inference_throughput_batches_per_ms_@1': 0.025664490830316914}\n"
     ]
    }
   ],
   "source": [
    "### To Complete ###\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": [\"\".join([chr(random.randint(97, 122)) for _ in range(100)]) for _ in range(1000)]\n",
    "})\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(dataset, seed=42)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "smash_config = SmashConfig()\n",
    "smash_config.add_tokenizer(model_id)\n",
    "smash_config.add_data(\n",
    "    (train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"text_generation_collate\"\n",
    ")\n",
    "smash_config['device'] = 'cuda'\n",
    "smash_config['quantizer'] = 'quanto'\n",
    "smash_config['quanto_weight_bits'] = \"qint4\"\n",
    "smash_config['recoverer'] = \"text_to_text_inplace_perp\"\n",
    "# smash_config['recoverer'] = \"text_to_text_perp\"\n",
    "\n",
    "results = smash_evaluate_perplexity_time(model, tokenizer, smash_config, dataset=\"WikiText\")\n",
    "print(results)\n",
    "### End of To Complete ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Finetune LLM quality with out-of-distribution data\n",
    "\n",
    "**Implementation task:**\n",
    "- Finetune in-place or by adding paramters a quantized LLM with Quanto and evaluate its quality and latency metric on the WikiText dataset.\n",
    "\n",
    "**Questions:**\n",
    "- Is there a performance improvement after finetuning?\n",
    "- Do you observe a latency difference? How could you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f8538f52dd0>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "INFO - Verifying Pruna token.\n",
      "INFO - You have used 360 hours this month.\n",
      "INFO - Starting quantizer quanto...\n",
      "INFO - quantizer quanto was applied successfully.\n",
      "INFO - Starting recoverer text_to_text_inplace_perp...\n",
      "INFO - You have used 360 hours this month.\n",
      "WARNING - Skipped 2.57e+07 head parameters that were not trainable due to quantization.\n",
      "INFO - Recovering with PERP: norm + bias, totaling 3.19e+05 parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b66f7d8e2841529c0384450cdd2848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6097d262778140ce8a2e98f34769d63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb60a3f165d54ee58017b0032bc48be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f558290dbdd4ddf8ecc62fb50d6007e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>4.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>4.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>3.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.761500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - recoverer text_to_text_inplace_perp was applied successfully.\n",
      "INFO - You have used 360 hours this month.\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Testing compatibility with functools.partial(<function text_generation_collate at 0x7f8538f52dd0>, tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, max_seq_len=None)...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity_y_gt': 70.56623840332031, 'inference_elapsed_time_ms_@1': 3814.258159637451, 'inference_latency_ms_@1': 38.14258159637451, 'inference_throughput_batches_per_ms_@1': 0.026217417860752535}\n"
     ]
    }
   ],
   "source": [
    "### To Complete ###\n",
    "train_ds = load_dataset(\"SamuelYang/bookcorpus\")[\"train\"]\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(train_ds, seed=42)\n",
    "train_ds = train_ds.select(range(1000))\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "smash_config = SmashConfig()\n",
    "smash_config.add_tokenizer(model_id)\n",
    "smash_config.add_data(\n",
    "    (train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"text_generation_collate\"\n",
    ")\n",
    "smash_config['device'] = 'cuda'\n",
    "smash_config['quantizer'] = 'quanto'\n",
    "smash_config['quanto_weight_bits'] = \"qint4\"\n",
    "smash_config['recoverer'] = \"text_to_text_inplace_perp\"\n",
    "# smash_config['recoverer'] = \"text_to_text_perp\"\n",
    "\n",
    "results = smash_evaluate_perplexity_time(model, tokenizer, smash_config, dataset=\"WikiText\")\n",
    "print(results)\n",
    "### End of To Complete ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
